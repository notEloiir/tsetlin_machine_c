{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5435ac62",
   "metadata": {},
   "source": [
    "# Noisy-XOR Dataset Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3e3458",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eb12404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceec3b3c",
   "metadata": {},
   "source": [
    "## Generate the Noisy-XOR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "261beef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_noisy_xor_dataset(\n",
    "    n_samples, n_features, n_xor_features, noise_level, random_state=None\n",
    "):\n",
    "    rng = np.random.RandomState(random_state)\n",
    "\n",
    "    # Generate random binary features\n",
    "    X = rng.randint(0, 2, size=(n_samples, n_features))\n",
    "\n",
    "    # Ensure n_xor_features is not greater than n_features\n",
    "    if n_xor_features > n_features:\n",
    "        raise ValueError(\"n_xor_features cannot be greater than n_features\")\n",
    "\n",
    "    # Calculate y_clean based on the XOR of the first n_xor_features\n",
    "    if n_xor_features == 0:\n",
    "        # If no features are designated for XOR, y_clean could be all zeros or random\n",
    "        # For simplicity, let's make it random if n_xor_features is 0\n",
    "        y_clean = rng.randint(0, 2, size=n_samples)\n",
    "    else:\n",
    "        y_clean = X[:, 0]\n",
    "        for i in range(1, n_xor_features):\n",
    "            y_clean = np.logical_xor(y_clean, X[:, i])\n",
    "        y_clean = y_clean.astype(int)\n",
    "\n",
    "    # Introduce noise\n",
    "    n_noise = int(noise_level * n_samples)\n",
    "    noise_indices = rng.choice(n_samples, size=n_noise, replace=False)\n",
    "\n",
    "    y = np.copy(y_clean)\n",
    "    y[noise_indices] = 1 - y[noise_indices]  # Flip the bits\n",
    "\n",
    "    return X.astype(np.uint8), y.astype(np.uint32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae9746cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Information:\n",
      "X: (1000, 16), [0 1]\n",
      "y: (1000,), [0 1]\n",
      "X: [0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0], y: 1\n",
      "X: [1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0], y: 0\n",
      "X: [1 1 1 0 1 0 0 0 0 0 1 1 1 1 1 0], y: 0\n",
      "X: [1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 0], y: 0\n",
      "X: [0 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1], y: 1\n",
      "Training set class proportions:\n",
      "  Class 0: 0.53, Class 1: 0.47\n",
      "Testing set class proportions:\n",
      "  Class 0: 0.53, Class 1: 0.47\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Generate the dataset with specified parameters\n",
    "X_data, y_data = generate_noisy_xor_dataset(\n",
    "    n_samples=1000, n_features=16, n_xor_features=2, noise_level=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Display dataset information\n",
    "print(\"Dataset Information:\")\n",
    "print(f\"X: {X_data.shape}, {np.unique(X_data)}\")\n",
    "print(f\"y: {y_data.shape}, {np.unique(y_data)}\")\n",
    "\n",
    "\n",
    "# Display the first N_DISPLAY_SAMPLES samples\n",
    "N_DISPLAY_SAMPLES = 5\n",
    "for x_sample, y_sample in zip(X_data[:N_DISPLAY_SAMPLES], y_data[:N_DISPLAY_SAMPLES]):\n",
    "    print(f\"X: {x_sample}, y: {y_sample}\")\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train_np, X_test_np, y_train, y_test = train_test_split(\n",
    "    X_data, y_data, test_size=0.2, stratify=y_data, random_state=42\n",
    ")\n",
    "\n",
    "# Define feature names\n",
    "n_features = X_data.shape[1]\n",
    "feature_names = [f\"feature_{i}\" for i in range(n_features)]\n",
    "\n",
    "# Convert NumPy arrays to Pandas DataFrames\n",
    "X_train = pd.DataFrame(X_train_np, columns=feature_names)\n",
    "X_test = pd.DataFrame(X_test_np, columns=feature_names)\n",
    "\n",
    "# Print proportions of classes in training and testing sets\n",
    "print(\"Training set class proportions:\")\n",
    "print(f\"  Class 0: {np.mean(y_train == 0):.2f}, Class 1: {np.mean(y_train == 1):.2f}\")\n",
    "print(\"Testing set class proportions:\")\n",
    "print(f\"  Class 0: {np.mean(y_test == 0):.2f}, Class 1: {np.mean(y_test == 1):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd55b770",
   "metadata": {},
   "source": [
    "## Evaluate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49c2fbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from time import perf_counter\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test, name=None):\n",
    "    # Fit the model\n",
    "    start_time = perf_counter()\n",
    "    model.fit(X_train, y_train)\n",
    "    training_time = perf_counter() - start_time\n",
    "\n",
    "    # Estimate the model size\n",
    "    model_size = None\n",
    "    try:\n",
    "        if name in (\n",
    "            \"Green Tsetlin Classifier\",\n",
    "            \"Green Tsetlin Sparse Classifier\",\n",
    "            \"C Tsetlin Classifier\",\n",
    "            \"GridSearch C Tsetlin Classifier\",\n",
    "            \"C Tsetlin Sparse Classifier\",\n",
    "            \"GridSearch C Tsetlin Sparse Classifier\",\n",
    "        ):\n",
    "            if isinstance(model, GridSearchCV):\n",
    "                model_size = model.best_estimator_.estimate_model_size()\n",
    "            else:\n",
    "                model_size = model.estimate_model_size()\n",
    "        else:\n",
    "            pickled_model = pickle.dumps(model)\n",
    "            model_size = len(pickled_model)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Predict on the training set\n",
    "    start_time = perf_counter()\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    prediction_train_time = perf_counter() - start_time\n",
    "\n",
    "    # Predict on the test set\n",
    "    start_time = perf_counter()\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    prediction_test_time = perf_counter() - start_time\n",
    "\n",
    "    # Calculate Accuracy\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "    # print(f\"Model: {name}\")\n",
    "    # print(f\"  Training Accuracy: {train_accuracy:.4f}\")\n",
    "    # print(f\"  Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    if isinstance(model, GridSearchCV):\n",
    "        print(f\"Best parameters for {name}: \\n  {model.best_params_}\")\n",
    "\n",
    "    return (\n",
    "        name,\n",
    "        train_accuracy,\n",
    "        test_accuracy,\n",
    "        training_time,\n",
    "        prediction_train_time,\n",
    "        prediction_test_time,\n",
    "        model_size,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1291690b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated 1/18: Logistic Regression\n",
      "Evaluated 2/18: Decision Tree\n",
      "Evaluated 3/18: Random Forest\n",
      "Evaluated 4/18: MLP Classifier\n",
      "Evaluated 5/18: SVC\n",
      "Evaluated 6/18: Linear SVC\n",
      "Evaluated 7/18: K-Nearest Neighbors\n",
      "Evaluated 8/18: Gaussian Naive Bayes\n",
      "Evaluated 9/18: Gradient Boosting\n",
      "Evaluated 10/18: LightGBM\n",
      "Best parameters for GridSearch Random Forest: \n",
      "  {'max_depth': None, 'min_samples_split': 10, 'n_estimators': 50}\n",
      "Evaluated 11/18: GridSearch Random Forest\n",
      "Best parameters for GridSearch MLP Classifier: \n",
      "  {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (100,)}\n",
      "Evaluated 12/18: GridSearch MLP Classifier\n",
      "Evaluated 13/18: Green Tsetlin Classifier\n",
      "Evaluated 14/18: Green Tsetlin Sparse Classifier\n",
      "Evaluated 15/18: C Tsetlin Classifier\n",
      "Best parameters for GridSearch C Tsetlin Classifier: \n",
      "  {'boost_true_positive_feedback': False, 'num_clauses': 64, 's': 3}\n",
      "Evaluated 16/18: GridSearch C Tsetlin Classifier\n",
      "Evaluated 17/18: C Tsetlin Sparse Classifier\n",
      "Best parameters for GridSearch C Tsetlin Sparse Classifier: \n",
      "  {'boost_true_positive_feedback': False, 'num_clauses': 64, 's': 3}\n",
      "Evaluated 18/18: GridSearch C Tsetlin Sparse Classifier\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from lightgbm import LGBMClassifier\n",
    "from tsetlin_machine_py.green_tsetlin_clf import GreenTsetlinClassifier\n",
    "from tsetlin_machine_py.green_tsetlin_sparse_clf import GreenTsetlinSparseClassifier\n",
    "from tsetlin_machine_py.c_tsetlin_clf import CTsetlinClassifier\n",
    "from tsetlin_machine_py.c_tsetlin_sparse_clf import CTsetlinSparseClassifier\n",
    "\n",
    "models = [\n",
    "    (LogisticRegression(max_iter=1000, random_state=42), \"Logistic Regression\"),\n",
    "    (DecisionTreeClassifier(random_state=42), \"Decision Tree\"),\n",
    "    (RandomForestClassifier(random_state=42), \"Random Forest\"),\n",
    "    (MLPClassifier(max_iter=1000, solver=\"lbfgs\", random_state=42), \"MLP Classifier\"),\n",
    "    (SVC(random_state=42), \"SVC\"),\n",
    "    (LinearSVC(random_state=42, dual=\"auto\"), \"Linear SVC\"),  # type: ignore\n",
    "    (KNeighborsClassifier(), \"K-Nearest Neighbors\"),\n",
    "    (GaussianNB(), \"Gaussian Naive Bayes\"),\n",
    "    (GradientBoostingClassifier(random_state=42), \"Gradient Boosting\"),\n",
    "    (LGBMClassifier(random_state=42, verbose=-1), \"LightGBM\"),\n",
    "]\n",
    "\n",
    "# GridSearchCV for RandomForest\n",
    "param_grid_rf = {\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "    \"max_depth\": [None, 10, 20, 30],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "}\n",
    "grid_search_rf = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_grid_rf,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "models.append((grid_search_rf, \"GridSearch Random Forest\"))\n",
    "\n",
    "# GridSearchCV for MLPClassifier\n",
    "param_grid_mlp = {\n",
    "    \"hidden_layer_sizes\": [(50,), (100,), (50, 50)],\n",
    "    \"activation\": [\"relu\", \"tanh\"],\n",
    "    \"alpha\": [0.0001, 0.001, 0.01],\n",
    "}\n",
    "grid_search_mlp = GridSearchCV(\n",
    "    MLPClassifier(max_iter=1000, solver=\"lbfgs\", random_state=42),\n",
    "    param_grid_mlp,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "models.append((grid_search_mlp, \"GridSearch MLP Classifier\"))\n",
    "\n",
    "# Green Tsetlin Classifier\n",
    "green_tsetlin_clf = GreenTsetlinClassifier(random_state=42)\n",
    "models.append((green_tsetlin_clf, \"Green Tsetlin Classifier\"))\n",
    "\n",
    "# Green Tsetlin Sparse Classifier\n",
    "green_tsetlin_sparse_clf = GreenTsetlinSparseClassifier(random_state=42)\n",
    "models.append((green_tsetlin_sparse_clf, \"Green Tsetlin Sparse Classifier\"))\n",
    "\n",
    "# C Tsetlin Classifier\n",
    "c_tsetlin_clf = CTsetlinClassifier(random_state=42)\n",
    "models.append((c_tsetlin_clf, \"C Tsetlin Classifier\"))\n",
    "\n",
    "# GridSearchCV for C Tsetlin Classifier\n",
    "param_grid_ctsetlin = {\n",
    "    \"num_clauses\": [64, 125, 500],\n",
    "    \"boost_true_positive_feedback\": [False, True],\n",
    "    \"s\": [3, 9],\n",
    "}\n",
    "grid_search_ctsetlin = GridSearchCV(\n",
    "    CTsetlinClassifier(random_state=42),\n",
    "    param_grid_ctsetlin,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "models.append((grid_search_ctsetlin, \"GridSearch C Tsetlin Classifier\"))\n",
    "\n",
    "# C Tsetlin Sparse Classifier\n",
    "c_tsetlin_sparse_clf = CTsetlinSparseClassifier(random_state=42)\n",
    "models.append((c_tsetlin_sparse_clf, \"C Tsetlin Sparse Classifier\"))\n",
    "\n",
    "# GridSearchCV for C Tsetlin Sparse Classifier\n",
    "param_grid_ctsetlin_sparse = {\n",
    "    \"num_clauses\": [64, 125, 500],\n",
    "    \"boost_true_positive_feedback\": [False, True],\n",
    "    \"s\": [3, 9],\n",
    "}\n",
    "grid_search_ctsetlin_sparse = GridSearchCV(\n",
    "    CTsetlinSparseClassifier(random_state=42),\n",
    "    param_grid_ctsetlin_sparse,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "models.append((grid_search_ctsetlin_sparse, \"GridSearch C Tsetlin Sparse Classifier\"))\n",
    "\n",
    "# Evaluate each model\n",
    "results_table = PrettyTable()\n",
    "results_table.field_names = [\n",
    "    \"Model\",\n",
    "    \"Training Accuracy\",\n",
    "    \"Test Accuracy\",\n",
    "    \"Training Time (s)\",\n",
    "    \"Prediction Train Time (s)\",\n",
    "    \"Prediction Test Time (s)\",\n",
    "    \"Model Size (bytes)\",\n",
    "]\n",
    "for i, (model, name) in enumerate(models):\n",
    "    metrics = evaluate_model(model, X_train, y_train, X_test, y_test, name=name)\n",
    "    results_table.add_row(\n",
    "        [\n",
    "            metrics[0],\n",
    "            f\"{metrics[1]:.4f}\",\n",
    "            f\"{metrics[2]:.4f}\",\n",
    "            f\"{metrics[3]:.4f}\",\n",
    "            f\"{metrics[4]:.4f}\",\n",
    "            f\"{metrics[5]:.4f}\",\n",
    "            f\"{metrics[6]:,}\" if metrics[6] else \"N/A\",\n",
    "        ]\n",
    "    )\n",
    "    print(f\"Evaluated {i + 1}/{len(models)}: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "825d13dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Training Time (s)</th>\n",
       "      <th>Prediction Train Time (s)</th>\n",
       "      <th>Prediction Test Time (s)</th>\n",
       "      <th>Model Size (bytes)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.5487</td>\n",
       "      <td>0.4750</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>1,122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.9962</td>\n",
       "      <td>0.7350</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>33,536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.9962</td>\n",
       "      <td>0.8700</td>\n",
       "      <td>0.0790</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>4,127,036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLP Classifier</td>\n",
       "      <td>0.9962</td>\n",
       "      <td>0.8150</td>\n",
       "      <td>0.1067</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>35,897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.8950</td>\n",
       "      <td>0.9300</td>\n",
       "      <td>0.0192</td>\n",
       "      <td>0.0146</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>104,823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>0.5450</td>\n",
       "      <td>0.4750</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>1,006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>0.8250</td>\n",
       "      <td>0.7650</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>0.0217</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>20,179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Gaussian Naive Bayes</td>\n",
       "      <td>0.5513</td>\n",
       "      <td>0.4700</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>1,373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.8962</td>\n",
       "      <td>0.9150</td>\n",
       "      <td>0.0767</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>139,263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.9850</td>\n",
       "      <td>0.8950</td>\n",
       "      <td>0.0502</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>350,233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GridSearch Random Forest</td>\n",
       "      <td>0.9200</td>\n",
       "      <td>0.9150</td>\n",
       "      <td>2.9232</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>622,791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>GridSearch MLP Classifier</td>\n",
       "      <td>0.9962</td>\n",
       "      <td>0.8500</td>\n",
       "      <td>1.8754</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>39,978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Green Tsetlin Classifier</td>\n",
       "      <td>0.8925</td>\n",
       "      <td>0.9300</td>\n",
       "      <td>0.2858</td>\n",
       "      <td>0.0069</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>38,008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Green Tsetlin Sparse Classifier</td>\n",
       "      <td>0.8925</td>\n",
       "      <td>0.9300</td>\n",
       "      <td>1.7496</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>807,608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>C Tsetlin Classifier</td>\n",
       "      <td>0.8900</td>\n",
       "      <td>0.9300</td>\n",
       "      <td>0.3476</td>\n",
       "      <td>0.0147</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>43,008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>GridSearch C Tsetlin Classifier</td>\n",
       "      <td>0.8925</td>\n",
       "      <td>0.9300</td>\n",
       "      <td>0.4142</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>2,760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>C Tsetlin Sparse Classifier</td>\n",
       "      <td>0.8450</td>\n",
       "      <td>0.9050</td>\n",
       "      <td>0.4636</td>\n",
       "      <td>0.0141</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>144,352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>GridSearch C Tsetlin Sparse Classifier</td>\n",
       "      <td>0.8925</td>\n",
       "      <td>0.9300</td>\n",
       "      <td>0.4473</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>9,144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Model Training Accuracy Test Accuracy  \\\n",
       "0                      Logistic Regression            0.5487        0.4750   \n",
       "1                            Decision Tree            0.9962        0.7350   \n",
       "2                            Random Forest            0.9962        0.8700   \n",
       "3                           MLP Classifier            0.9962        0.8150   \n",
       "4                                      SVC            0.8950        0.9300   \n",
       "5                               Linear SVC            0.5450        0.4750   \n",
       "6                      K-Nearest Neighbors            0.8250        0.7650   \n",
       "7                     Gaussian Naive Bayes            0.5513        0.4700   \n",
       "8                        Gradient Boosting            0.8962        0.9150   \n",
       "9                                 LightGBM            0.9850        0.8950   \n",
       "10                GridSearch Random Forest            0.9200        0.9150   \n",
       "11               GridSearch MLP Classifier            0.9962        0.8500   \n",
       "12                Green Tsetlin Classifier            0.8925        0.9300   \n",
       "13         Green Tsetlin Sparse Classifier            0.8925        0.9300   \n",
       "14                    C Tsetlin Classifier            0.8900        0.9300   \n",
       "15         GridSearch C Tsetlin Classifier            0.8925        0.9300   \n",
       "16             C Tsetlin Sparse Classifier            0.8450        0.9050   \n",
       "17  GridSearch C Tsetlin Sparse Classifier            0.8925        0.9300   \n",
       "\n",
       "   Training Time (s) Prediction Train Time (s) Prediction Test Time (s)  \\\n",
       "0             0.0028                    0.0005                   0.0003   \n",
       "1             0.0020                    0.0004                   0.0004   \n",
       "2             0.0790                    0.0068                   0.0032   \n",
       "3             0.1067                    0.0008                   0.0004   \n",
       "4             0.0192                    0.0146                   0.0040   \n",
       "5             0.0013                    0.0008                   0.0004   \n",
       "6             0.0008                    0.0217                   0.0062   \n",
       "7             0.0012                    0.0009                   0.0004   \n",
       "8             0.0767                    0.0017                   0.0006   \n",
       "9             0.0502                    0.0016                   0.0007   \n",
       "10            2.9232                    0.0030                   0.0015   \n",
       "11            1.8754                    0.0008                   0.0003   \n",
       "12            0.2858                    0.0069                   0.0016   \n",
       "13            1.7496                    0.0031                   0.0010   \n",
       "14            0.3476                    0.0147                   0.0039   \n",
       "15            0.4142                    0.0013                   0.0006   \n",
       "16            0.4636                    0.0141                   0.0038   \n",
       "17            0.4473                    0.0009                   0.0005   \n",
       "\n",
       "   Model Size (bytes)  \n",
       "0               1,122  \n",
       "1              33,536  \n",
       "2           4,127,036  \n",
       "3              35,897  \n",
       "4             104,823  \n",
       "5               1,006  \n",
       "6              20,179  \n",
       "7               1,373  \n",
       "8             139,263  \n",
       "9             350,233  \n",
       "10            622,791  \n",
       "11             39,978  \n",
       "12             38,008  \n",
       "13            807,608  \n",
       "14             43,008  \n",
       "15              2,760  \n",
       "16            144,352  \n",
       "17              9,144  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the results table\n",
    "results_table.sortby = \"Test Accuracy\"\n",
    "results_table.reversesort = True\n",
    "# print(results_table)\n",
    "\n",
    "# Get data from PrettyTable\n",
    "columns = results_table.field_names\n",
    "data = results_table.rows\n",
    "\n",
    "# Create Pandas DataFrame\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# print(\"\\nResults for Noisy-XOR dataset:\")\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
