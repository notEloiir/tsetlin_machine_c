{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5435ac62",
   "metadata": {},
   "source": [
    "# Noisy-XOR Dataset Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3e3458",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eb12404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceec3b3c",
   "metadata": {},
   "source": [
    "## Generate the Noisy-XOR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "261beef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_noisy_xor_dataset(\n",
    "    n_samples, n_features, n_xor_features, noise_level, random_state=None\n",
    "):\n",
    "    rng = np.random.RandomState(random_state)\n",
    "\n",
    "    # Generate random binary features\n",
    "    X = rng.randint(0, 2, size=(n_samples, n_features))\n",
    "\n",
    "    # Ensure n_xor_features is not greater than n_features\n",
    "    if n_xor_features > n_features:\n",
    "        raise ValueError(\"n_xor_features cannot be greater than n_features\")\n",
    "\n",
    "    # Calculate y_clean based on the XOR of the first n_xor_features\n",
    "    if n_xor_features == 0:\n",
    "        # If no features are designated for XOR, y_clean could be all zeros or random\n",
    "        # For simplicity, let's make it random if n_xor_features is 0\n",
    "        y_clean = rng.randint(0, 2, size=n_samples)\n",
    "    else:\n",
    "        y_clean = X[:, 0]\n",
    "        for i in range(1, n_xor_features):\n",
    "            y_clean = np.logical_xor(y_clean, X[:, i])\n",
    "        y_clean = y_clean.astype(int)\n",
    "\n",
    "    # Introduce noise\n",
    "    n_noise = int(noise_level * n_samples)\n",
    "    noise_indices = rng.choice(n_samples, size=n_noise, replace=False)\n",
    "\n",
    "    y = np.copy(y_clean)\n",
    "    y[noise_indices] = 1 - y[noise_indices]  # Flip the bits\n",
    "\n",
    "    return X.astype(np.uint8), y.astype(np.uint32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae9746cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Information:\n",
      "X: (1000, 16), [0 1]\n",
      "y: (1000,), [0 1]\n",
      "X: [0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0], y: 1\n",
      "X: [1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0], y: 0\n",
      "X: [1 1 1 0 1 0 0 0 0 0 1 1 1 1 1 0], y: 0\n",
      "X: [1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 0], y: 0\n",
      "X: [0 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1], y: 1\n",
      "Training set class proportions:\n",
      "  Class 0: 0.53, Class 1: 0.47\n",
      "Testing set class proportions:\n",
      "  Class 0: 0.53, Class 1: 0.47\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Generate the dataset with specified parameters\n",
    "X_data, y_data = generate_noisy_xor_dataset(\n",
    "    n_samples=1000, n_features=16, n_xor_features=2, noise_level=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Display dataset information\n",
    "print(\"Dataset Information:\")\n",
    "print(f\"X: {X_data.shape}, {np.unique(X_data)}\")\n",
    "print(f\"y: {y_data.shape}, {np.unique(y_data)}\")\n",
    "\n",
    "\n",
    "# Display the first N_DISPLAY_SAMPLES samples\n",
    "N_DISPLAY_SAMPLES = 5\n",
    "for x_sample, y_sample in zip(X_data[:N_DISPLAY_SAMPLES], y_data[:N_DISPLAY_SAMPLES]):\n",
    "    print(f\"X: {x_sample}, y: {y_sample}\")\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train_np, X_test_np, y_train, y_test = train_test_split(\n",
    "    X_data, y_data, test_size=0.2, stratify=y_data, random_state=42\n",
    ")\n",
    "\n",
    "# Define feature names\n",
    "n_features = X_data.shape[1]\n",
    "feature_names = [f\"feature_{i}\" for i in range(n_features)]\n",
    "\n",
    "# Convert NumPy arrays to Pandas DataFrames\n",
    "X_train = pd.DataFrame(X_train_np, columns=feature_names)\n",
    "X_test = pd.DataFrame(X_test_np, columns=feature_names)\n",
    "\n",
    "# Print proportions of classes in training and testing sets\n",
    "print(\"Training set class proportions:\")\n",
    "print(f\"  Class 0: {np.mean(y_train == 0):.2f}, Class 1: {np.mean(y_train == 1):.2f}\")\n",
    "print(\"Testing set class proportions:\")\n",
    "print(f\"  Class 0: {np.mean(y_test == 0):.2f}, Class 1: {np.mean(y_test == 1):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd55b770",
   "metadata": {},
   "source": [
    "## Evaluate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1291690b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: [0 1]\n",
      "Batch 1: Train Accuracy: 0.5413, Test Accuracy: 0.5350\n",
      "Batch 2: Train Accuracy: 0.5212, Test Accuracy: 0.4850\n",
      "Batch 3: Train Accuracy: 0.4725, Test Accuracy: 0.4700\n",
      "Batch 4: Train Accuracy: 0.4850, Test Accuracy: 0.4950\n",
      "Batch 5: Train Accuracy: 0.5000, Test Accuracy: 0.5200\n",
      "Batch 6: Train Accuracy: 0.5500, Test Accuracy: 0.5450\n",
      "Batch 7: Train Accuracy: 0.4763, Test Accuracy: 0.4700\n",
      "Batch 8: Train Accuracy: 0.5625, Test Accuracy: 0.5350\n",
      "Batch 9: Train Accuracy: 0.5813, Test Accuracy: 0.6050\n",
      "Batch 10: Train Accuracy: 0.5663, Test Accuracy: 0.5600\n",
      "Batch 11: Train Accuracy: 0.5837, Test Accuracy: 0.5100\n",
      "Batch 12: Train Accuracy: 0.4838, Test Accuracy: 0.4800\n",
      "Batch 13: Train Accuracy: 0.5950, Test Accuracy: 0.5750\n",
      "Batch 14: Train Accuracy: 0.6012, Test Accuracy: 0.5850\n",
      "Batch 15: Train Accuracy: 0.5825, Test Accuracy: 0.5650\n",
      "Batch 16: Train Accuracy: 0.5875, Test Accuracy: 0.5800\n",
      "Batch 17: Train Accuracy: 0.5138, Test Accuracy: 0.5050\n",
      "Batch 18: Train Accuracy: 0.5750, Test Accuracy: 0.5900\n",
      "Batch 19: Train Accuracy: 0.6150, Test Accuracy: 0.6250\n",
      "Batch 20: Train Accuracy: 0.6725, Test Accuracy: 0.6600\n",
      "Batch 21: Train Accuracy: 0.6388, Test Accuracy: 0.6200\n",
      "Batch 22: Train Accuracy: 0.5800, Test Accuracy: 0.5800\n",
      "Batch 23: Train Accuracy: 0.5988, Test Accuracy: 0.6000\n",
      "Batch 24: Train Accuracy: 0.5437, Test Accuracy: 0.5400\n",
      "Batch 25: Train Accuracy: 0.6425, Test Accuracy: 0.6250\n",
      "Batch 26: Train Accuracy: 0.5500, Test Accuracy: 0.5550\n",
      "Batch 27: Train Accuracy: 0.7350, Test Accuracy: 0.6900\n",
      "Batch 28: Train Accuracy: 0.7588, Test Accuracy: 0.7450\n",
      "Batch 29: Train Accuracy: 0.7775, Test Accuracy: 0.7550\n",
      "Batch 30: Train Accuracy: 0.7812, Test Accuracy: 0.7700\n",
      "Batch 31: Train Accuracy: 0.7762, Test Accuracy: 0.7850\n",
      "Batch 32: Train Accuracy: 0.7600, Test Accuracy: 0.7350\n",
      "Batch 33: Train Accuracy: 0.7887, Test Accuracy: 0.8050\n",
      "Batch 34: Train Accuracy: 0.7875, Test Accuracy: 0.7950\n",
      "Batch 35: Train Accuracy: 0.8313, Test Accuracy: 0.8750\n",
      "Batch 36: Train Accuracy: 0.8237, Test Accuracy: 0.8200\n",
      "Batch 37: Train Accuracy: 0.7900, Test Accuracy: 0.7950\n",
      "Batch 38: Train Accuracy: 0.8425, Test Accuracy: 0.9000\n",
      "Batch 39: Train Accuracy: 0.8275, Test Accuracy: 0.8600\n",
      "Batch 40: Train Accuracy: 0.8650, Test Accuracy: 0.8950\n",
      "Batch 41: Train Accuracy: 0.7987, Test Accuracy: 0.8050\n",
      "Batch 42: Train Accuracy: 0.7675, Test Accuracy: 0.7750\n",
      "Batch 43: Train Accuracy: 0.8612, Test Accuracy: 0.9200\n",
      "Batch 44: Train Accuracy: 0.8325, Test Accuracy: 0.8700\n",
      "Batch 45: Train Accuracy: 0.8712, Test Accuracy: 0.9200\n",
      "Batch 46: Train Accuracy: 0.8850, Test Accuracy: 0.9300\n",
      "Batch 47: Train Accuracy: 0.8150, Test Accuracy: 0.8550\n",
      "Batch 48: Train Accuracy: 0.8888, Test Accuracy: 0.9250\n",
      "Batch 49: Train Accuracy: 0.8875, Test Accuracy: 0.9150\n",
      "Batch 50: Train Accuracy: 0.8925, Test Accuracy: 0.9300\n",
      "Batch 51: Train Accuracy: 0.8912, Test Accuracy: 0.9250\n",
      "Batch 52: Train Accuracy: 0.8912, Test Accuracy: 0.9150\n",
      "Batch 53: Train Accuracy: 0.7200, Test Accuracy: 0.7150\n",
      "Batch 54: Train Accuracy: 0.8925, Test Accuracy: 0.9300\n",
      "Batch 55: Train Accuracy: 0.8912, Test Accuracy: 0.9200\n",
      "Batch 56: Train Accuracy: 0.8925, Test Accuracy: 0.9300\n",
      "Batch 57: Train Accuracy: 0.8925, Test Accuracy: 0.9300\n",
      "Batch 58: Train Accuracy: 0.8925, Test Accuracy: 0.9300\n",
      "Batch 59: Train Accuracy: 0.8912, Test Accuracy: 0.9200\n",
      "Batch 60: Train Accuracy: 0.8925, Test Accuracy: 0.9300\n",
      "Batch 61: Train Accuracy: 0.8888, Test Accuracy: 0.9200\n",
      "Batch 62: Train Accuracy: 0.8562, Test Accuracy: 0.8700\n",
      "Batch 63: Train Accuracy: 0.8912, Test Accuracy: 0.9300\n",
      "Batch 64: Train Accuracy: 0.8925, Test Accuracy: 0.9300\n",
      "Batch 65: Train Accuracy: 0.8925, Test Accuracy: 0.9300\n",
      "Batch 66: Train Accuracy: 0.8938, Test Accuracy: 0.9300\n",
      "Batch 67: Train Accuracy: 0.8925, Test Accuracy: 0.9300\n",
      "Batch 68: Train Accuracy: 0.8900, Test Accuracy: 0.9200\n",
      "Batch 69: Train Accuracy: 0.8925, Test Accuracy: 0.9300\n",
      "Batch 70: Train Accuracy: 0.8925, Test Accuracy: 0.9300\n",
      "Batch 71: Train Accuracy: 0.8912, Test Accuracy: 0.9250\n",
      "Batch 72: Train Accuracy: 0.8925, Test Accuracy: 0.9300\n",
      "Batch 73: Train Accuracy: 0.8925, Test Accuracy: 0.9300\n",
      "Batch 74: Train Accuracy: 0.8925, Test Accuracy: 0.9300\n",
      "Batch 75: Train Accuracy: 0.8925, Test Accuracy: 0.9300\n",
      "Batch 76: Train Accuracy: 0.8925, Test Accuracy: 0.9300\n",
      "Batch 77: Train Accuracy: 0.8925, Test Accuracy: 0.9300\n",
      "Batch 78: Train Accuracy: 0.8925, Test Accuracy: 0.9300\n",
      "Batch 79: Train Accuracy: 0.8925, Test Accuracy: 0.9300\n",
      "Batch 80: Train Accuracy: 0.8925, Test Accuracy: 0.9300\n"
     ]
    }
   ],
   "source": [
    "from tsetlin_machine_py.c_tsetlin_clf import CTsetlinClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "batch_size = 10\n",
    "steps = 800\n",
    "\n",
    "assert X_train.shape[0] >= steps, (\n",
    "    f\"{batch_size=} * {steps=} exceeds training set size {X_train.shape[0]=}\"\n",
    ")\n",
    "\n",
    "c_tsetlin_clf = CTsetlinClassifier(random_state=42)\n",
    "classes = np.unique(y_train)\n",
    "print(f\"Classes: {classes}\")\n",
    "c_tsetlin_clf.init_empty_state(n_features=X_train.shape[1], classes=classes)\n",
    "\n",
    "for start_idx in range(0, steps, batch_size):\n",
    "    end_idx = min(start_idx + batch_size, X_train.shape[0])\n",
    "    X_train_batch = X_train.iloc[start_idx:end_idx]\n",
    "    y_train_batch = y_train[start_idx:end_idx]\n",
    "\n",
    "    c_tsetlin_clf.partial_fit(X_train_batch, y_train_batch, classes=classes, epochs=1)\n",
    "\n",
    "    y_train_pred = c_tsetlin_clf.predict(X_train)\n",
    "    y_test_pred = c_tsetlin_clf.predict(X_test)\n",
    "\n",
    "    print(\n",
    "        f\"Batch {start_idx // batch_size + 1}: \"\n",
    "        f\"Train Accuracy: {accuracy_score(y_train, y_train_pred):.4f}, \"\n",
    "        f\"Test Accuracy: {accuracy_score(y_test, y_test_pred):.4f}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
